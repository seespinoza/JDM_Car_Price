{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all models and metrics\n",
    "\n",
    "# Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Linear models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, Lasso, BayesianRidge, GammaRegressor\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "# Tree-based models\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "\n",
    "# Ensemble methods\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "Now that we have finished exploring the data, we have a good idea of the nuances of our dataset and which variables we want to use to predict car price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = pd.read_csv('data/modeling_data.csv')\n",
    "mdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Best Model\n",
    "\n",
    "We will be performing 10-fold cross validation and imputing missing values at each fold. The model with the best average RSME after 10 folds will be chosen as the best model. \n",
    "\n",
    "We have chosen RSME as the error metric for chosing the best model because it gives higher weight to larger errors, unlike MAE or MAPE. Our client is planning to use this model to estimate future costs, so it is important we eliminate models that have too large of errors and could severely underestimate future costs. MAE and MAPE will still be calculated for gaining a better understanding of model performance but will not be used for choosing the best-performing model.\n",
    "\n",
    "\n",
    "Additionally, we will not be considering interpretability when choosing models. Our client has informed us that they simply want the best performing model and do not care much about knowing how much each variable affects our predictions. In other words, we will most likely not be choosing a simpler modeling algorithm, such as linear regression, because it is more interpretable than an ensemble modeling algorithm like Random Forests.\n",
    "\n",
    "Before we can choose the best model, we need to create functions that will help us perform n-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that imputes missing values in a dataframe using the mean of each column\n",
    "def impute(df, imputer):\n",
    "    \n",
    "    # Impute missing vals\n",
    "    imputed_df = pd.DataFrame(imputer.fit_transform(df))\n",
    "    imputed_df.columns = df.columns\n",
    "    imputed_df.index = df.index\n",
    "    \n",
    "    return imputed_df\n",
    "\n",
    "# Function that splits a dataframe into 80/20 train and test sets\n",
    "def get_split(df):\n",
    "    \n",
    "    # Get predictors\n",
    "    X = df.drop('price', axis=1)\n",
    "    \n",
    "    # Get target variable\n",
    "    y = df.price\n",
    "    \n",
    "    # Split data 80/20\n",
    "    Xtrain, Xtest, ytrain, ytest = sklearn.model_selection.train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    # Create Imputer object\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    \n",
    "    # Fill missing values for predictors\n",
    "    Xtrain = impute(Xtrain, imp_mean)\n",
    "    Xtest = impute(Xtest, imp_mean)\n",
    "    \n",
    "    return Xtrain, Xtest, ytrain, ytest\n",
    "\n",
    "def baseline_model(train, Xtest):\n",
    "    \n",
    "    # List of all body types in dataset\n",
    "    body_types = ['sedan', 'minivan', 'truck', 'SUV', 'hatchback',\n",
    "       'convertible', 'coupe', 'mini_vehicle']\n",
    "    \n",
    "    # Initialize price column\n",
    "    Xtest['price'] = 0\n",
    "    \n",
    "    # Calculate average price for each body type\n",
    "    for body_type in body_types:\n",
    "        \n",
    "        # Calculate mean for a body type\n",
    "        body_mean = train[train[body_type]==1]['price'].mean()\n",
    "        \n",
    "        # Set average as predction in test set\n",
    "        Xtest.loc[Xtest[body_type] == 1, 'price'] = body_mean\n",
    "    \n",
    "    return Xtest.price\n",
    "        \n",
    "\n",
    "# Function that performs n-fold cross validations for a single model and\n",
    "# Calculates error metrics for each fold\n",
    "def cross_validate(df, m=None, n=10):\n",
    "    \n",
    "    # Dictionary with error metrics\n",
    "    metric_dict = {'MAE': [], 'MAPE': [], 'RSME': []}\n",
    "    \n",
    "    for fold in range(n):\n",
    "        \n",
    "        # Split data for current fold\n",
    "        X_train, X_test, y_train, y_test = get_split(df)\n",
    "        \n",
    "        # Baseline model\n",
    "        if m is None:\n",
    "            \n",
    "            X_train['price'] = y_train\n",
    "            \n",
    "            y_pred = baseline_model(X_train, X_test)\n",
    "        \n",
    "        # fit non-baseline models\n",
    "        else:\n",
    "            m.fit(X_train, y_train)\n",
    "        \n",
    "            # predict test\n",
    "            y_pred = m.predict(X_test)\n",
    "        \n",
    "        \n",
    "        # Calculate metrics\n",
    "        metric_dict['MAE'].append(mean_absolute_error(y_test, y_pred))\n",
    "        \n",
    "        metric_dict['MAPE'].append(np.mean(np.absolute(y_test - y_pred) / y_test))\n",
    "        \n",
    "        metric_dict['RSME'].append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        \n",
    "    return metric_dict\n",
    "\n",
    "# Function that performs n-fold cross validations for an ensemble model and\n",
    "# Calculates error metrics for each fold\n",
    "def cross_validate_ensemble(df, m_list, n=10):\n",
    "    \n",
    "    # Dictionary with error metrics\n",
    "    metric_dict = {'MAE': [], 'MAPE': [], 'RSME': []}\n",
    "    \n",
    "    for fold in range(n):\n",
    "        \n",
    "        # Split data for current fold\n",
    "        X_train, X_test, y_train, y_test = get_split(df)\n",
    "        \n",
    "        y_preds = []\n",
    "        for m in m_list:\n",
    "            # fit model\n",
    "            m.fit(X_train, y_train)\n",
    "        \n",
    "            # predict test\n",
    "            y_pred = m.predict(X_test)\n",
    "            y_preds.append(y_pred)\n",
    "        \n",
    "        # Predictions are calculated as the average prediction of\n",
    "        # all models in the ensemble\n",
    "        y_pred = np.average(y_preds, axis=0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metric_dict['MAE'].append(mean_absolute_error(y_test, y_pred))\n",
    "        \n",
    "        metric_dict['MAPE'].append(np.mean(np.absolute(y_test - y_pred) / y_test))\n",
    "        \n",
    "        metric_dict['RSME'].append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        \n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Algorithms\n",
    "\n",
    "We have chosen several different modeling algorithms to test on our dataset. Although our selection does not represent the entirety of models that are offered by the sklearn library, we have chosen a mix of linear and non-linear models that should give us an insight on how to best predict car price.\n",
    "\n",
    "Additionaly, we will be testing a baseline model that makes predictions using a very simple method: using the average price of each `body_type` as the predicted price. Sometimes, very simple models perform just as well as very complex models, and it may not be worth implementing a model that is computationally-intensive due to lack of resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [LinearRegression(), Ridge(), SGDRegressor(), Lasso(), BayesianRidge(), GammaRegressor(),\n",
    "             LinearSVR(), DecisionTreeRegressor(), ExtraTreeRegressor(), AdaBoostRegressor(), \n",
    "              GradientBoostingRegressor(), RandomForestRegressor(), XGBRegressor()]\n",
    "\n",
    "# Dataframe for storing all modeling results\n",
    "model_res_df = pd.DataFrame({'Model': [],\n",
    "                             'MAE': [],\n",
    "                             'MAPE': [],\n",
    "                             'RSME': []\n",
    "                            })\n",
    "\n",
    "# Generate performance results for each regressor in regressors list\n",
    "for model in regressors:\n",
    "    \n",
    "    # 10-fold cross validation results\n",
    "    current_results = cross_validate(mdf, model, 10)\n",
    "    \n",
    "    # Append Results to dataframe\n",
    "    model_res_df = pd.concat([model_res_df,\n",
    "                             pd.DataFrame({'Model': [type(model).__name__ for i in range(10)],\n",
    "                                           'MAE': [i for i in current_results['MAE']],\n",
    "                                           'MAPE': [i for i in current_results['MAPE']],\n",
    "                                           'RSME': [i for i in current_results['RSME']]\n",
    "                                          })])\n",
    "\n",
    "# Generate performance results for ensemble model\n",
    "current_results = cross_validate_ensemble(mdf, [DecisionTreeRegressor(), RandomForestRegressor(), XGBRegressor()], 10)\n",
    "\n",
    "# Append model results to dataframe\n",
    "model_res_df = pd.concat([model_res_df,\n",
    "                            pd.DataFrame({'Model': ['Ensemble'] * 10,\n",
    "                                          'MAE': [i for i in current_results['MAE']],\n",
    "                                          'MAPE': [i for i in current_results['MAPE']],\n",
    "                                          'RSME': [i for i in current_results['RSME']]\n",
    "                                          })])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate performance results for baseline model\n",
    "current_results = cross_validate(mdf)\n",
    "\n",
    "# Append model results to dataframe\n",
    "model_res_df = pd.concat([model_res_df,\n",
    "                            pd.DataFrame({'Model': ['Baseline'] * 10,\n",
    "                                          'MAE': [i for i in current_results['MAE']],\n",
    "                                          'MAPE': [i for i in current_results['MAPE']],\n",
    "                                          'RSME': [i for i in current_results['RSME']]\n",
    "                                          })])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with average error metrics for each model\n",
    "mean_model_res = model_res_df.groupby(['Model']).mean().reset_index()\n",
    "\n",
    "# Entries by RSME\n",
    "mean_model_res.sort_values('RSME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig_scores_of_models = plt.figure('Model Performance',figsize=(5,5))\n",
    "\n",
    "new_res = mean_model_res[(mean_model_res['Model'] != 'SGDRegressor') & (mean_model_res['Model'] != 'LinearSVR')]\n",
    "new_res = new_res.sort_values('RSME', ascending=False)\n",
    "\n",
    "\n",
    "sns.pointplot(x=new_res.Model, y=new_res.RSME, markers=['o'], linestyles=['-'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Model Scores')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results\n",
    "\n",
    "Our Random Forest outperformed all models after 10-fold cross validation since it has the lowest mean RSME value. However, the Random Forest model does have a slightly MAE and MAPE value than the Ensemble model. We suspect this is due to RSME penalizing larger errors more heavily; in other words, the Ensemble model had a lower MAE but contained larger errors which drove its RSME higher.\n",
    "\n",
    "Also, the Random Forest model performed significantly better than our baseline, indicating that choosing a more complex model is likely worth the extra resources.\n",
    "\n",
    "Interestingly, our Ensemble model composed of XGBoost, Decision Trees, and Random Forest came in second. This is most likely due to the errors of each model in the ensemble not being correlated with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure('Distribution of RandomForest RSME',figsize=(5,5))\n",
    "\n",
    "plt.hist(model_res_df[model_res_df['Model']=='RandomForestRegressor'].RSME, bins=20)\n",
    "plt.title('Distribution of RandomForest RSME')\n",
    "plt.xlabel('RSME')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation RSME Distribution\n",
    "\n",
    "\n",
    "Above, we can see the distribution of the RSME for all 10 folds. \n",
    "\n",
    "Our average RSME was around 13,000, but you can see that for some folds we got an RSME as big as 17,000. The performance we expect from our model in production will likely be somewhere between 17,000 and 11,000 RSME.\n",
    "\n",
    "We would likely see a normal distribution if we were to perform more than 10 folds. We could then calculate the standard deviation of our distribution and give rough estimates of how likely certain ranges of RSME values would be in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for current fold\n",
    "X_train, X_test, y_train, y_test = get_split(mdf)\n",
    "\n",
    "# Model\n",
    "m = RandomForestRegressor()\n",
    "\n",
    "# Fit model\n",
    "m.fit(X_train, y_train)\n",
    "        \n",
    "# Predict test\n",
    "y_pred = m.predict(X_test)\n",
    "\n",
    "fig = plt.figure('RF Residual vs Actuals',figsize=(5,5))\n",
    "\n",
    "plt.scatter(y_test, y_pred - y_test)\n",
    "plt.title('RF Residual vs Actuals')\n",
    "plt.xlabel('Actuals')\n",
    "plt.ylabel('Residuals')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals vs Actuals\n",
    "(Not to be confused with residuals vs fitted values plot for linear models)\n",
    "\n",
    "In general, it seems that our residuals are negatively correlated with our actuals, aside from a couple of outliers. In other words, as our actuals get bigger, we tend to more and more underprect our target variable. This especially becomes an issue when we reach extreme actuals such as \\\\$8M where our residuals are also around \\\\$8M.\n",
    "\n",
    "In future iterations of the model we may need to break out training sets into cars of different price ranges since our model does not handle extremely expensive cars well. This means we could have a \"high cost\" model and a \"low cost\" model.\n",
    "\n",
    "We may also want to try modeling techniques with a loss function that more heavily penalizes underestimation rather than overestimation, since our client is more concerned with the former.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_importances = pd.Series(m.feature_importances_, index=X_train.columns)\n",
    "\n",
    "fig_fi = plt.figure('Feature Importance using MDI',figsize=(5,5))\n",
    "\n",
    "forest_importances.plot.bar()\n",
    "plt.title('Feature Importance using Mean Decrease in Impurity')\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('MDI')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Importance\n",
    "\n",
    "We calculated the importance of each variable in the Random Forest model using Mean Decrease in Impurity (MDI). Although this method has been shown to not always be reliable, we will use it to gain a rough understanding of our variables.\n",
    "\n",
    "Surprisingly, `body_type` did not seem to be a very important variable. This may suggest that every body type in our dataset has a broad price range. Our three most important variables were `engine _cc`, `km`, and `registration_year`, respectively. Collecting more information on engine specifications and the year the car was produced might help improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "\n",
    "We will train the RandomForest on all data and save the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputer\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Train model on all data\n",
    "final_model = RandomForestRegressor()\n",
    "\n",
    "#Impute missing values on entire dataset\n",
    "X_imputed = impute(mdf, imp_mean)\n",
    "y = mdf.price\n",
    "\n",
    "# Train model on entire dataset\n",
    "final_model.fit(X_imputed, y)\n",
    "\n",
    "# Write model to file\n",
    "pickle.dump(final_model, open('models/JDM_RF.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
